{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e962839c-275d-4448-9497-2c27401e7d95",
   "metadata": {},
   "source": [
    "##### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f657f6b-31a4-4eca-b50c-c437a4c2aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dtale\n",
      "  Downloading dtale-3.16.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting dash-colorscales (from dtale)\n",
      "  Downloading dash_colorscales-0.0.4.tar.gz (62 kB)\n",
      "     ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "     -------------------------- ------------- 41.0/62.3 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 62.3/62.3 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting dash-daq (from dtale)\n",
      "  Downloading dash_daq-0.5.0.tar.gz (642 kB)\n",
      "     ---------------------------------------- 0.0/642.7 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 225.3/642.7 kB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  634.9/642.7 kB 8.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 642.7/642.7 kB 6.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future>=0.14.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (0.18.3)\n",
      "Collecting missingno (from dtale)\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: pandas in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2.1.4)\n",
      "Collecting squarify (from dtale)\n",
      "  Downloading squarify-0.4.4-py3-none-any.whl.metadata (600 bytes)\n",
      "Collecting strsimpy (from dtale)\n",
      "  Downloading strsimpy-0.2.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: six in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (1.16.0)\n",
      "Collecting xlrd (from dtale)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4!=4.13.0b2 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2024.8.30)\n",
      "Collecting dash-bootstrap-components (from dtale)\n",
      "  Downloading dash_bootstrap_components-1.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lz4 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (4.3.2)\n",
      "Requirement already satisfied: cycler in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (0.11.0)\n",
      "Collecting dash (from dtale)\n",
      "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (0.12.2)\n",
      "Requirement already satisfied: werkzeug in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2.2.3)\n",
      "Collecting Flask-Compress (from dtale)\n",
      "  Downloading Flask_Compress-1.17-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (1.2.2)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (0.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (1.26.4)\n",
      "Requirement already satisfied: openpyxl!=3.2.0b1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (3.0.10)\n",
      "Requirement already satisfied: xarray in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2023.6.0)\n",
      "Requirement already satisfied: scipy!=1.12.0rc1,!=1.14.0,!=1.14.0rc1,!=1.14.0rc2 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (1.11.4)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (1.1.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (5.9.0)\n",
      "Requirement already satisfied: Flask in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2.2.5)\n",
      "Requirement already satisfied: itsdangerous in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (2.31.0)\n",
      "Collecting kaleido (from dtale)\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dtale) (3.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from beautifulsoup4!=4.13.0b2->dtale) (2.5)\n",
      "Collecting dash-html-components==2.0.0 (from dash->dtale)\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dash-core-components==2.0.0 (from dash->dtale)\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting dash-table==5.0.0 (from dash->dtale)\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dash->dtale) (7.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dash->dtale) (4.9.0)\n",
      "Collecting retrying (from dash->dtale)\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dash->dtale) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from dash->dtale) (68.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from Flask->dtale) (3.1.3)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from Flask->dtale) (8.1.7)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from plotly->dtale) (8.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from werkzeug->dtale) (2.1.3)\n",
      "Requirement already satisfied: brotli in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from Flask-Compress->dtale) (1.0.9)\n",
      "Requirement already satisfied: zstandard in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from Flask-Compress->dtale) (0.19.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from matplotlib->dtale) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from pandas->dtale) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from pandas->dtale) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from requests->dtale) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from requests->dtale) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from requests->dtale) (2.0.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from scikit-learn->dtale) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from scikit-learn->dtale) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from statsmodels->dtale) (0.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from click>=8.0->Flask->dtale) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\av hanumantha rao\\anaconda3\\lib\\site-packages (from importlib-metadata->dash->dtale) (3.17.0)\n",
      "Downloading dtale-3.16.1-py2.py3-none-any.whl (14.7 MB)\n",
      "   ---------------------------------------- 0.0/14.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.7 MB 15.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.1/14.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/14.7 MB 12.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.5/14.7 MB 12.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.2/14.7 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.9/14.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.7/14.7 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.5/14.7 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.1/14.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.0/14.7 MB 14.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.0/14.7 MB 15.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.8/14.7 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.7/14.7 MB 15.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.7/14.7 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.7/14.7 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.2/14.7 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.7 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.7 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.7/14.7 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/7.8 MB 31.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.1/7.8 MB 26.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.1/7.8 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.1/7.8 MB 25.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.1/7.8 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.3/7.8 MB 25.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.8 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.8 MB 23.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Downloading dash_bootstrap_components-1.7.1-py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 229.3/229.3 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading Flask_Compress-1.17-py3-none-any.whl (8.7 kB)\n",
      "Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "   ---------------------------------------- 0.0/65.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.9/65.9 MB 18.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.8/65.9 MB 23.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 3.1/65.9 MB 24.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 4.3/65.9 MB 27.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 5.3/65.9 MB 26.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 6.4/65.9 MB 25.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 7.5/65.9 MB 25.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 8.4/65.9 MB 25.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 9.2/65.9 MB 25.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 10.0/65.9 MB 24.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 10.7/65.9 MB 24.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 11.6/65.9 MB 24.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 12.3/65.9 MB 23.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 13.1/65.9 MB 22.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 13.8/65.9 MB 21.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 14.7/65.9 MB 21.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 15.5/65.9 MB 20.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 16.3/65.9 MB 20.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 17.1/65.9 MB 19.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 17.9/65.9 MB 19.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 17.9/65.9 MB 19.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 17.9/65.9 MB 19.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 18.2/65.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 19.9/65.9 MB 17.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 20.8/65.9 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 22.4/65.9 MB 19.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 23.1/65.9 MB 18.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 24.1/65.9 MB 19.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 25.0/65.9 MB 18.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 25.8/65.9 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 26.6/65.9 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 27.6/65.9 MB 18.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 28.5/65.9 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 29.4/65.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 30.1/65.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 31.1/65.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 32.0/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 32.9/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 33.9/65.9 MB 19.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 34.8/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 35.6/65.9 MB 18.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 36.7/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.7/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 38.7/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 39.6/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 40.3/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 41.2/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 42.1/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 43.1/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 44.0/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 45.0/65.9 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 45.9/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 46.7/65.9 MB 19.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 47.6/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 48.5/65.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 49.5/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 50.4/65.9 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 51.4/65.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 52.3/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 53.3/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 54.2/65.9 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 55.0/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 55.7/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 56.5/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 57.3/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 58.0/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 58.8/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 59.6/65.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 60.6/65.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 61.6/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 62.5/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 63.5/65.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  64.5/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.2/65.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 65.9/65.9 MB 13.1 MB/s eta 0:00:00\n",
      "Downloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Downloading squarify-0.4.4-py3-none-any.whl (4.1 kB)\n",
      "Downloading strsimpy-0.2.1-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.5/96.5 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: dash-colorscales, dash-daq\n",
      "  Building wheel for dash-colorscales (setup.py): started\n",
      "  Building wheel for dash-colorscales (setup.py): finished with status 'done'\n",
      "  Created wheel for dash-colorscales: filename=dash_colorscales-0.0.4-py3-none-any.whl size=62573 sha256=2a09e9ffe8c5c7f26f98c31e5b458dfbbcaa1727b7d3a6c819c3a51fb890bbba\n",
      "  Stored in directory: c:\\users\\av hanumantha rao\\appdata\\local\\pip\\cache\\wheels\\a8\\c3\\fd\\eaaa499af5a62e4f31c6e9ee0a32c05ece15176c4ea4d68398\n",
      "  Building wheel for dash-daq (setup.py): started\n",
      "  Building wheel for dash-daq (setup.py): finished with status 'done'\n",
      "  Created wheel for dash-daq: filename=dash_daq-0.5.0-py3-none-any.whl size=669719 sha256=3badbb83796e2a511a0deb8cdb658f62a44b7edc3796edd337856cecd9c2633b\n",
      "  Stored in directory: c:\\users\\av hanumantha rao\\appdata\\local\\pip\\cache\\wheels\\9a\\e1\\a3\\ef7c3fa914e4df214fdcb64529c44669e3e72ebdb784db10e0\n",
      "Successfully built dash-colorscales dash-daq\n",
      "Installing collected packages: strsimpy, squarify, kaleido, dash-table, dash-html-components, dash-core-components, dash-colorscales, xlrd, retrying, Flask-Compress, dash, missingno, dash-daq, dash-bootstrap-components, dtale\n",
      "Successfully installed Flask-Compress-1.17 dash-2.18.2 dash-bootstrap-components-1.7.1 dash-colorscales-0.0.4 dash-core-components-2.0.0 dash-daq-0.5.0 dash-html-components-2.0.0 dash-table-5.0.0 dtale-3.16.1 kaleido-0.2.1 missingno-0.5.2 retrying-1.3.4 squarify-0.4.4 strsimpy-0.2.1 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53174288-b0a1-413f-877b-8ee5e5c6bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b3fa36-3f76-4f7f-b403-ad8ac2d62cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading\n",
    "train = pd.read_csv('train_v9rqX0R.csv')\n",
    "test = pd.read_csv('test_AbJTz2l.csv')\n",
    "submission = pd.read_csv('sample_submission_8RXa3c6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5c2874-4b95-4ce2-b120-55f9c64fb93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (8523, 12)\n",
      "Test data shape: (5681, 11)\n",
      "\n",
      "Train data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                7060 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                6113 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n",
      "None\n",
      "\n",
      "Test data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5681 entries, 0 to 5680\n",
      "Data columns (total 11 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            5681 non-null   object \n",
      " 1   Item_Weight                4705 non-null   float64\n",
      " 2   Item_Fat_Content           5681 non-null   object \n",
      " 3   Item_Visibility            5681 non-null   float64\n",
      " 4   Item_Type                  5681 non-null   object \n",
      " 5   Item_MRP                   5681 non-null   float64\n",
      " 6   Outlet_Identifier          5681 non-null   object \n",
      " 7   Outlet_Establishment_Year  5681 non-null   int64  \n",
      " 8   Outlet_Size                4075 non-null   object \n",
      " 9   Outlet_Location_Type       5681 non-null   object \n",
      " 10  Outlet_Type                5681 non-null   object \n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 488.3+ KB\n",
      "None\n",
      "\n",
      "Missing values in train data:\n",
      "Item_Identifier                 0\n",
      "Item_Weight                  1463\n",
      "Item_Fat_Content                0\n",
      "Item_Visibility                 0\n",
      "Item_Type                       0\n",
      "Item_MRP                        0\n",
      "Outlet_Identifier               0\n",
      "Outlet_Establishment_Year       0\n",
      "Outlet_Size                  2410\n",
      "Outlet_Location_Type            0\n",
      "Outlet_Type                     0\n",
      "Item_Outlet_Sales               0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "Item_Identifier                 0\n",
      "Item_Weight                   976\n",
      "Item_Fat_Content                0\n",
      "Item_Visibility                 0\n",
      "Item_Type                       0\n",
      "Item_MRP                        0\n",
      "Outlet_Identifier               0\n",
      "Outlet_Establishment_Year       0\n",
      "Outlet_Size                  1606\n",
      "Outlet_Location_Type            0\n",
      "Outlet_Type                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. Initial Data Exploration\n",
    "print(\"Train data shape:\", train.shape)\n",
    "print(\"Test data shape:\", test.shape)\n",
    "print(\"\\nTrain data info:\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\nTest data info:\")\n",
    "print(test.info())\n",
    "\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7faae8-0915-4199-8d15-793653623898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                7060 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                6113 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9bb4f9-debe-46af-a92f-1735df511865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7060.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.857645</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>140.992782</td>\n",
       "      <td>1997.831867</td>\n",
       "      <td>2181.288914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.643456</td>\n",
       "      <td>0.051598</td>\n",
       "      <td>62.275067</td>\n",
       "      <td>8.371760</td>\n",
       "      <td>1706.499616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.555000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.290000</td>\n",
       "      <td>1985.000000</td>\n",
       "      <td>33.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.773750</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>93.826500</td>\n",
       "      <td>1987.000000</td>\n",
       "      <td>834.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>143.012800</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1794.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.850000</td>\n",
       "      <td>0.094585</td>\n",
       "      <td>185.643700</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>3101.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.350000</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>266.888400</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>13086.964800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Item_Weight  Item_Visibility     Item_MRP  Outlet_Establishment_Year  \\\n",
       "count  7060.000000      8523.000000  8523.000000                8523.000000   \n",
       "mean     12.857645         0.066132   140.992782                1997.831867   \n",
       "std       4.643456         0.051598    62.275067                   8.371760   \n",
       "min       4.555000         0.000000    31.290000                1985.000000   \n",
       "25%       8.773750         0.026989    93.826500                1987.000000   \n",
       "50%      12.600000         0.053931   143.012800                1999.000000   \n",
       "75%      16.850000         0.094585   185.643700                2004.000000   \n",
       "max      21.350000         0.328391   266.888400                2009.000000   \n",
       "\n",
       "       Item_Outlet_Sales  \n",
       "count        8523.000000  \n",
       "mean         2181.288914  \n",
       "std          1706.499616  \n",
       "min            33.290000  \n",
       "25%           834.247400  \n",
       "50%          1794.331000  \n",
       "75%          3101.296400  \n",
       "max         13086.964800  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d990e60a-bb62-4560-9a60-0e88d477777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7060.000000\n",
      "mean       12.857645\n",
      "std         4.643456\n",
      "min         4.555000\n",
      "25%         8.773750\n",
      "50%        12.600000\n",
      "75%        16.850000\n",
      "max        21.350000\n",
      "Name: Item_Weight, dtype: float64\n",
      "count    4705.000000\n",
      "mean       12.695633\n",
      "std         4.664849\n",
      "min         4.555000\n",
      "25%         8.645000\n",
      "50%        12.500000\n",
      "75%        16.700000\n",
      "max        21.350000\n",
      "Name: Item_Weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train['Item_Weight'].describe())\n",
    "print(test['Item_Weight'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95806666-197f-4e86-985e-07ff3a3dc2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Weight'].fillna(train['Item_Weight'].mean(),inplace=True)  #replacing null values with mean values\n",
    "test['Item_Weight'].fillna(test['Item_Weight'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e02303c-4a7f-4124-b94e-7729be09640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier                 0\n",
       "Item_Weight                     0\n",
       "Item_Fat_Content                0\n",
       "Item_Visibility                 0\n",
       "Item_Type                       0\n",
       "Item_MRP                        0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Size                  2410\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Type                     0\n",
       "Item_Outlet_Sales               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e3cddd2-c1ba-400f-bc6c-cad25c9fac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8523.000000\n",
      "mean       12.857645\n",
      "std         4.226124\n",
      "min         4.555000\n",
      "25%         9.310000\n",
      "50%        12.857645\n",
      "75%        16.000000\n",
      "max        21.350000\n",
      "Name: Item_Weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train['Item_Weight'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f604ccf8-4be8-407b-b6f1-540b287b3929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Medium\n",
       "1       Medium\n",
       "2       Medium\n",
       "3          NaN\n",
       "4         High\n",
       "         ...  \n",
       "8518      High\n",
       "8519       NaN\n",
       "8520     Small\n",
       "8521    Medium\n",
       "8522     Small\n",
       "Name: Outlet_Size, Length: 8523, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Outlet_Size']  #it is a categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42ff2bd-728f-4d4b-b22d-cffc3c3e2e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outlet_Size\n",
       "Medium    2793\n",
       "Small     2388\n",
       "High       932\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Outlet_Size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ca96bc-2cc6-440f-a21e-4b654e92a372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Medium\n",
       "Name: Outlet_Size, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Outlet_Size'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d0e44c8-7b2d-4a77-90a8-9c4b453f7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0],inplace=True)\n",
    "test['Outlet_Size'].fillna(test['Outlet_Size'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbd2d28-4bfa-44d2-9027-35b023820c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              0\n",
       "Item_Weight                  0\n",
       "Item_Fat_Content             0\n",
       "Item_Visibility              0\n",
       "Item_Type                    0\n",
       "Item_MRP                     0\n",
       "Outlet_Identifier            0\n",
       "Outlet_Establishment_Year    0\n",
       "Outlet_Size                  0\n",
       "Outlet_Location_Type         0\n",
       "Outlet_Type                  0\n",
       "Item_Outlet_Sales            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e461184-25cc-43a2-b265-0a674a6b1cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              0\n",
       "Item_Weight                  0\n",
       "Item_Fat_Content             0\n",
       "Item_Visibility              0\n",
       "Item_Type                    0\n",
       "Item_MRP                     0\n",
       "Outlet_Identifier            0\n",
       "Outlet_Establishment_Year    0\n",
       "Outlet_Size                  0\n",
       "Outlet_Location_Type         0\n",
       "Outlet_Type                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af0fda6-5cc8-4597-93c7-23988b00149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Preprocessing and Feature Engineering\n",
    "# Save identifiers for later\n",
    "train_identifiers = train[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "test_identifiers = test[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "\n",
    "# Combine train and test for preprocessing\n",
    "test['Item_Outlet_Sales'] = 0  # temporary column for test data\n",
    "combined = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46fc5734-b30a-4709-9b52-8ae9e8a84dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# 3.1 Extract item category from Item_Identifier\n",
    "combined['Item_Category'] = combined['Item_Identifier'].str[:2]\n",
    "combined['Item_Category'] = combined['Item_Category'].map({\n",
    "    'FD': 'Food',\n",
    "    'DR': 'Drinks',\n",
    "    'NC': 'Non-Consumable'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94a2d5f-58db-4145-abd0-c70965a5397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Calculate outlet age\n",
    "combined['Outlet_Years'] = 2013 - combined['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab3ea13-ece3-4335-9dca-bf8a40507de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Normalize Item_Fat_Content values\n",
    "fat_content_map = {\n",
    "    'Low Fat': 'Low Fat',\n",
    "    'Regular': 'Regular',\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}\n",
    "combined['Item_Fat_Content'] = combined['Item_Fat_Content'].map(fat_content_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2b84935-91b0-4530-adc2-4f9ac1bcaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Non-consumables should not have fat content\n",
    "combined.loc[combined['Item_Category'] == 'Non-Consumable', 'Item_Fat_Content'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70446e27-4aad-4870-8b7f-52ad9433e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Item_Visibility - log transform to handle skewness\n",
    "combined['Item_Visibility_Log'] = np.log1p(combined['Item_Visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42502faf-90df-4ee0-9db7-00d66d7b2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Item_MRP - create price segments\n",
    "combined['Item_MRP_Segment'] = pd.qcut(combined['Item_MRP'], 4, labels=['Economy', 'Standard', 'Premium', 'Super Premium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75aaa52a-4f1e-4751-9092-7785be3a452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Encoding Categorical Variables\n",
    "# Use label encoding for ordinal features and one-hot for nominal\n",
    "# Label Encoding for ordinal variables\n",
    "label_encoders = {}\n",
    "for col in ['Outlet_Size', 'Item_MRP_Segment']:\n",
    "    le = LabelEncoder()\n",
    "    combined[col] = le.fit_transform(combined[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a356db1-e11a-4346-af32-112d5838bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.8 One-hot encoding for nominal variables\n",
    "# We'll use one-hot encoding for selected columns\n",
    "nominal_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Category']\n",
    "combined = pd.get_dummies(combined, columns=nominal_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5f49910-b3d7-4c1c-8115-c071d8b5abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split back into train and test\n",
    "train_processed = combined[combined['Item_Outlet_Sales'] > 0].drop('Item_Identifier', axis=1)\n",
    "test_processed = combined[combined['Item_Outlet_Sales'] == 0].drop('Item_Outlet_Sales', axis=1)\n",
    "test_ids = test_processed['Item_Identifier']\n",
    "test_processed = test_processed.drop('Item_Identifier', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1b38f-7e19-4c73-ab5e-837e41be6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "310f24a2-ffe7-48a8-8e0e-d229b1a1e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d03d6d4-a9be-4966-a665-589ebcce0c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://DESKTOP-M00UVD3:40000/dtale/iframe/1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x229976af290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtale.show(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3563a9d1-4fe1-4426-8d05-956afebea0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'OUT049'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m     11\u001b[0m                                  min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:810\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    809\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 810\u001b[0m     array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(new_dtype)\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    812\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    420\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'OUT049'"
     ]
    }
   ],
   "source": [
    "# 6. Model Training\n",
    "# Extract features and target\n",
    "X = train_processed.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train_processed['Item_Outlet_Sales']\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_split=5, \n",
    "                                 min_samples_leaf=2, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fca9bca8-b55f-4b0c-9c0f-e879fba4bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features used in model:\n",
      "['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Size', 'Outlet_Years', 'Item_Visibility_Log', 'Item_MRP_Segment', 'Item_Fat_Content_1', 'Item_Fat_Content_2', 'Item_Type_1', 'Item_Type_2', 'Item_Type_3', 'Item_Type_4', 'Item_Type_5', 'Item_Type_6', 'Item_Type_7', 'Item_Type_8', 'Item_Type_9', 'Item_Type_10', 'Item_Type_11', 'Item_Type_12', 'Item_Type_13', 'Item_Type_14', 'Item_Type_15', 'Outlet_Location_Type_1', 'Outlet_Location_Type_2', 'Outlet_Type_1', 'Outlet_Type_2', 'Outlet_Type_3', 'Item_Category_1', 'Item_Category_2']\n",
      "\n",
      "Data types in training data:\n",
      "Item_Weight               float64\n",
      "Item_Visibility           float64\n",
      "Item_MRP                  float64\n",
      "Outlet_Size                 int32\n",
      "Outlet_Years                int64\n",
      "Item_Visibility_Log       float64\n",
      "Item_MRP_Segment            int32\n",
      "Item_Fat_Content_1           bool\n",
      "Item_Fat_Content_2           bool\n",
      "Item_Type_1                  bool\n",
      "Item_Type_2                  bool\n",
      "Item_Type_3                  bool\n",
      "Item_Type_4                  bool\n",
      "Item_Type_5                  bool\n",
      "Item_Type_6                  bool\n",
      "Item_Type_7                  bool\n",
      "Item_Type_8                  bool\n",
      "Item_Type_9                  bool\n",
      "Item_Type_10                 bool\n",
      "Item_Type_11                 bool\n",
      "Item_Type_12                 bool\n",
      "Item_Type_13                 bool\n",
      "Item_Type_14                 bool\n",
      "Item_Type_15                 bool\n",
      "Outlet_Location_Type_1       bool\n",
      "Outlet_Location_Type_2       bool\n",
      "Outlet_Type_1                bool\n",
      "Outlet_Type_2                bool\n",
      "Outlet_Type_3                bool\n",
      "Item_Category_1              bool\n",
      "Item_Category_2              bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 7. Model Training\n",
    "# Extract features and target\n",
    "X = train_processed.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train_processed['Item_Outlet_Sales']\n",
    "\n",
    "print(\"\\nFeatures used in model:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "print(\"\\nData types in training data:\")\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9728d258-7b41-4c7d-bc16-45b5449ad894",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                 min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_split=5, \n",
    "                                min_samples_leaf=2, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa3aeb-9407-4578-ba46-154c2888a7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
